{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Cairo University Faculty of Engineering\n",
    "## Deep Learning \n",
    "## Assignment 1 \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please write your full name here\n",
    "- **Name** : \"amr mohamed ali esmail\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part1: Numpy and Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise gives you a brief introduction to Python. Even if you've used Python before, this will help familiarize you with functions we'll need.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- You will be using Python 3.\n",
    "- Avoid using for-loops and while-loops, unless you are explicitly told to do so.\n",
    "- Do not modify the (# GRADED FUNCTION [function name]) comment in some cells. Your work would not be graded if you change this. Each cell containing that comment should only contain one function.\n",
    "- After coding your function, run the cell right below it to check if your result is correct.\n",
    "\n",
    "**After this part you will:**\n",
    "\n",
    "- Be able to use iPython Notebooks\n",
    "- Be able to use numpy functions and numpy matrix/vector operations\n",
    "- Understand the concept of \"broadcasting\"\n",
    "- Be able to vectorize code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You only need to write code between the ### START CODE HERE ### and ### END CODE HERE ### comments.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1 - Building basic functions with numpy ##\n",
    "\n",
    "### 1.1 - sigmoid function ###\n",
    "\n",
    "**Exercise**: Build a function that returns the sigmoid of a real number x. Use math.exp(x) for the exponential function.\n",
    "\n",
    "**Reminder**:\n",
    "$sigmoid(x) = \\frac{1}{1+e^{-x}}$ is sometimes also known as the logistic function. It is a non-linear function used not only in Machine Learning (Logistic Regression), but also in Deep Learning.\n",
    "\n",
    "<img src=\"./images/Sigmoid.png\" style=\"width:500px;height:228px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: basic_sigmoid\n",
    "\n",
    "import math\n",
    "\n",
    "def basic_sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute sigmoid of x.\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    s = 1 / (1 + (math.exp(-1 * x)))\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9525741268224334"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_sigmoid(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, we rarely use the \"math\" library in deep learning because the inputs of the functions are real numbers. In deep learning we mostly use matrices and vectors. This is why numpy is more useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be real number, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\sbme\\fourth year\\second term\\DeepL\\ass1\\A1_tf_basics.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/sbme/fourth%20year/second%20term/DeepL/ass1/A1_tf_basics.ipynb#ch0000009?line=0'>1</a>\u001b[0m \u001b[39m### One reason why we use \"numpy\" instead of \"math\" in Deep Learning ###\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/sbme/fourth%20year/second%20term/DeepL/ass1/A1_tf_basics.ipynb#ch0000009?line=1'>2</a>\u001b[0m x \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/sbme/fourth%20year/second%20term/DeepL/ass1/A1_tf_basics.ipynb#ch0000009?line=2'>3</a>\u001b[0m basic_sigmoid(x)\n",
      "\u001b[1;32md:\\sbme\\fourth year\\second term\\DeepL\\ass1\\A1_tf_basics.ipynb Cell 7'\u001b[0m in \u001b[0;36mbasic_sigmoid\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/sbme/fourth%20year/second%20term/DeepL/ass1/A1_tf_basics.ipynb#ch0000006?line=5'>6</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/sbme/fourth%20year/second%20term/DeepL/ass1/A1_tf_basics.ipynb#ch0000006?line=6'>7</a>\u001b[0m \u001b[39mCompute sigmoid of x.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/sbme/fourth%20year/second%20term/DeepL/ass1/A1_tf_basics.ipynb#ch0000006?line=7'>8</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/sbme/fourth%20year/second%20term/DeepL/ass1/A1_tf_basics.ipynb#ch0000006?line=12'>13</a>\u001b[0m \u001b[39ms -- sigmoid(x)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/sbme/fourth%20year/second%20term/DeepL/ass1/A1_tf_basics.ipynb#ch0000006?line=13'>14</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/sbme/fourth%20year/second%20term/DeepL/ass1/A1_tf_basics.ipynb#ch0000006?line=15'>16</a>\u001b[0m \u001b[39m### START CODE HERE ### (≈ 1 line of code)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/sbme/fourth%20year/second%20term/DeepL/ass1/A1_tf_basics.ipynb#ch0000006?line=16'>17</a>\u001b[0m s \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m (math\u001b[39m.\u001b[39;49mexp(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m*\u001b[39;49m x)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/sbme/fourth%20year/second%20term/DeepL/ass1/A1_tf_basics.ipynb#ch0000006?line=18'>19</a>\u001b[0m \u001b[39m### END CODE HERE ###\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/sbme/fourth%20year/second%20term/DeepL/ass1/A1_tf_basics.ipynb#ch0000006?line=20'>21</a>\u001b[0m \u001b[39mreturn\u001b[39;00m s\n",
      "\u001b[1;31mTypeError\u001b[0m: must be real number, not list"
     ]
    }
   ],
   "source": [
    "### One reason why we use \"numpy\" instead of \"math\" in Deep Learning ###\n",
    "x = [1, 2, 3]\n",
    "basic_sigmoid(x) # you will see this give an error when you run it, because x is a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any time you need more info on a numpy function, we encourage you to look at [the official documentation](https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.exp.html). \n",
    "\n",
    "\n",
    "**Exercise**: Implement the sigmoid function using numpy. \n",
    "\n",
    "**Instructions**: x could now be either a real number, a vector, or a matrix. \n",
    "$$ \\text{For } x \\in \\mathbb{R}^n \\text{,     } sigmoid(x) = sigmoid\\begin{pmatrix}\n",
    "    x_1  \\\\\n",
    "    x_2  \\\\\n",
    "    ...  \\\\\n",
    "    x_n  \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    \\frac{1}{1+e^{-x_1}}  \\\\\n",
    "    \\frac{1}{1+e^{-x_2}}  \\\\\n",
    "    ...  \\\\\n",
    "    \\frac{1}{1+e^{-x_n}}  \\\\\n",
    "\\end{pmatrix}\\tag{1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "import numpy as np # this means you can access numpy functions by writing np.function() instead of numpy.function()\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    s = 1 / (1 + (np.exp(-1 * x)))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73105858, 0.88079708, 0.95257413])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Sigmoid gradient\n",
    "\n",
    "As you've seen, you will need to compute gradients to optimize loss functions. Let's code your first gradient function.\n",
    "\n",
    "**Exercise**: Implement the function sigmoid_grad() to compute the gradient of the sigmoid function with respect to its input x. The formula is: $$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}$$\n",
    "You often code this function in two steps:\n",
    "1. Set s to be the sigmoid of x. You might find your sigmoid(x) function useful.\n",
    "2. Compute $\\sigma'(x) = s(1-s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid_derivative\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.\n",
    "    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array\n",
    "\n",
    "    Return:\n",
    "    ds -- Your computed gradient.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    s = sigmoid(x)\n",
    "    ds = s * (1 - s)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid_derivative(x) = [0.19661193 0.10499359 0.04517666]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "print (\"sigmoid_derivative(x) = \" + str(sigmoid_derivative(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Reshaping arrays ###\n",
    "\n",
    "Two common numpy functions used in deep learning are [np.shape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html) and [np.reshape()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html). \n",
    "- X.shape is used to get the shape (dimension) of a matrix/vector X. \n",
    "- X.reshape(...) is used to reshape X into some other dimension. \n",
    "\n",
    "For example, in computer science, an image is represented by a 3D array of shape $(length, height, depth = 3)$. However, when you read an image as the input of an algorithm you convert it to a vector of shape $(length*height*3, 1)$. In other words, you \"unroll\", or reshape, the 3D array into a 1D vector.\n",
    "\n",
    "<img src=\"./images/image2vector_kiank.png\" style=\"width:500px;height:300;\">\n",
    "\n",
    "**Exercise**: Implement `image2vector()` that takes an input of shape (length, height, 3) and returns a vector of shape (length\\*height\\*3, 1). For example, if you would like to reshape an array v of shape (a, b, c) into a vector of shape (a*b,c) you would do:\n",
    "``` python\n",
    "v = v.reshape((v.shape[0]*v.shape[1], v.shape[2])) # v.shape[0] = a ; v.shape[1] = b ; v.shape[2] = c\n",
    "```\n",
    "- Please don't hardcode the dimensions of image as a constant. Instead look up the quantities you need with `image.shape[0]`, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: image2vector\n",
    "def image2vector(image):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    image -- a numpy array of shape (length, height, depth)\n",
    "    \n",
    "    Returns:\n",
    "    v -- a vector of shape (length*height*depth, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    v = image.reshape(image.shape[0]*image.shape[1]*image.shape[2],1)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image2vector(image) = [[0.67826139]\n",
      " [0.29380381]\n",
      " [0.90714982]\n",
      " [0.52835647]\n",
      " [0.4215251 ]\n",
      " [0.45017551]\n",
      " [0.92814219]\n",
      " [0.96677647]\n",
      " [0.85304703]\n",
      " [0.52351845]\n",
      " [0.19981397]\n",
      " [0.27417313]\n",
      " [0.60659855]\n",
      " [0.00533165]\n",
      " [0.10820313]\n",
      " [0.49978937]\n",
      " [0.34144279]\n",
      " [0.94630077]]\n"
     ]
    }
   ],
   "source": [
    "# This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y,3) where 3 represents the RGB values\n",
    "image = np.array([[[ 0.67826139,  0.29380381],\n",
    "        [ 0.90714982,  0.52835647],\n",
    "        [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "       [[ 0.92814219,  0.96677647],\n",
    "        [ 0.85304703,  0.52351845],\n",
    "        [ 0.19981397,  0.27417313]],\n",
    "\n",
    "       [[ 0.60659855,  0.00533165],\n",
    "        [ 0.10820313,  0.49978937],\n",
    "        [ 0.34144279,  0.94630077]]])\n",
    "print (\"image2vector(image) = \" + str(image2vector(image)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - Broadcasting and the softmax function ####\n",
    "A very important concept to understand in numpy is \"broadcasting\". It is very useful for performing mathematical operations between arrays of different shapes. For the full details on broadcasting, you can read the official [broadcasting documentation](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Implement a softmax function using numpy. You can think of softmax as a normalizing function used when your algorithm needs to classify two or more classes. You will learn more about softmax later in the course.\n",
    "\n",
    "**Instructions**:\n",
    "- $ \\text{for } x \\in \\mathbb{R}^{1\\times n} \\text{,     } softmax(x) = softmax(\\begin{bmatrix}\n",
    "    x_1  &&\n",
    "    x_2 &&\n",
    "    ...  &&\n",
    "    x_n  \n",
    "\\end{bmatrix}) = \\begin{bmatrix}\n",
    "     \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n",
    "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n",
    "    ...  &&\n",
    "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n",
    "\\end{bmatrix} $ \n",
    "\n",
    "- $\\text{for a matrix } x \\in \\mathbb{R}^{m \\times n} \\text{,  $x_{ij}$ maps to the element in the $i^{th}$ row and $j^{th}$ column of $x$, thus we have: }$  $$softmax(x) = softmax\\begin{bmatrix}\n",
    "    x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
    "    x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
    "\\end{bmatrix} = \\begin{pmatrix}\n",
    "    softmax\\text{(first row of x)}  \\\\\n",
    "    softmax\\text{(second row of x)} \\\\\n",
    "    ...  \\\\\n",
    "    softmax\\text{(last row of x)} \\\\\n",
    "\\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "Note that \"m\" is used to represent the \"number of training examples\".\n",
    "Softmax should be performed for all features of each training example, so softmax would be performed on the rows.\n",
    "\n",
    "$m$ is the number of rows and $n$ is the number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: softmax\n",
    "import numpy as np\n",
    "def softmax(x):\n",
    "    \"\"\"Calculates the softmax for each row of the input x.\n",
    "\n",
    "    Your code should work for a row vector and also for matrices of shape (m,n).\n",
    "\n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (m,n)\n",
    "\n",
    "    Returns:\n",
    "    s -- A numpy matrix equal to the softmax of x, of shape (m,n)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    # Apply exp() element-wise to x to get x_exp. \n",
    "    x_exp = np.exp(x)\n",
    "    # Create a vector x_sum that sums each row of x_exp. \n",
    "    x_sum = np.sum(x,axis=1)[:,np.newaxis]\n",
    "    # Compute softmax(x) by dividing results of 2 previous steps. \n",
    "    s = x_exp / x_sum\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax(x) = [[5.06442745e+02 4.61816006e-01 9.27582244e+00 6.25000000e-02\n",
      "  6.25000000e-02]\n",
      " [9.13860965e+01 1.23677633e+01 8.33333333e-02 8.33333333e-02\n",
      "  8.33333333e-02]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [9, 2, 5, 0, 0],\n",
    "    [7, 5, 0, 0 ,0]])\n",
    "print(\"softmax(x) = \" + str(softmax(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "- If you print the shapes of x_exp, x_sum and s above and rerun the assessment cell, you will see that x_sum is of shape (2,1) while x_exp and s are of shape (2,5). **x_exp/x_sum** works due to python broadcasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "**What you need to remember:**\n",
    "- np.exp(x) works for any np.array x and applies the exponential function to every coordinate\n",
    "- the sigmoid function and its gradient\n",
    "- image2vector is commonly used in deep learning\n",
    "- np.reshape is widely used. In the future, you'll see that keeping your matrix/vector dimensions straight will go toward eliminating a lot of bugs. \n",
    "- numpy has efficient built-in functions\n",
    "- broadcasting is extremely useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Implement the L1 and L2 loss functions\n",
    "\n",
    "**Exercise**: Implement the numpy vectorized version of the L1 loss. You may find the function abs(x) (absolute value of x) useful.\n",
    "\n",
    "**Reminder**:\n",
    "- The loss is used to evaluate the performance of your model. The bigger your loss is, the more different your predictions ($ \\hat{y} $) are from the true values ($y$). In deep learning, you use optimization algorithms like Gradient Descent to train your model and to minimize the cost.\n",
    "- L1 loss is defined as:\n",
    "$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L1\n",
    "def L1(yhat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- vector of size m (predicted labels)\n",
    "    y -- vector of size m (true labels)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the value of the L1 loss function defined above\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    loss = np.abs((yhat - y)).sum()\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 = 1.1\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L1 = \" + str(L1(yhat,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Implement the numpy vectorized version of the L2 loss. There are several way of implementing the L2 loss but you may find the function np.dot() useful. As a reminder, if $x = [x_1, x_2, ..., x_n]$, then `np.dot(x,x)` = $\\sum_{j=0}^n x_j^{2}$. \n",
    "\n",
    "- L2 loss is defined as $$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L2\n",
    "\n",
    "def L2(yhat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- vector of size m (predicted labels)\n",
    "    y -- vector of size m (true labels)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the value of the L2 loss function defined above\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    loss = np.square((yhat - y)).sum()\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 = 0.43\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L2 = \" + str(L2(yhat,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "**What to remember:**\n",
    "- Vectorization is very important in deep learning. It provides computational efficiency and clarity.\n",
    "- You have reviewed the L1 and L2 loss.\n",
    "- You are familiar with many numpy functions such as np.sum, np.dot, np.multiply, np.maximum, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57knM8jrYZ2t"
   },
   "source": [
    "# Part2: Intro to TensorFlow\n",
    "\n",
    "In this assignment, you'll get exposure to using TensorFlow and learn how it can be used for solving deep learning tasks. Go through the code and run each cell. Along the way, you'll encounter several ***TODO*** blocks -- follow the instructions to fill them out before running those cells and continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LkaimNJfYZ2w"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QNMcdP4m3Vs"
   },
   "source": [
    "## 1.1 Why is TensorFlow called TensorFlow?\n",
    "\n",
    "TensorFlow is called 'TensorFlow' because it handles the flow (node/mathematical operation) of Tensors, which are data structures that you can think of as multi-dimensional arrays. Tensors are represented as n-dimensional arrays of base dataypes such as a string or integer -- they provide a way to generalize vectors and matrices to higher dimensions.\n",
    "\n",
    "The ```shape``` of a Tensor defines its number of dimensions and the size of each dimension. The ```rank``` of a Tensor provides the number of dimensions (n-dimensions) -- you can also think of this as the Tensor's order or degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tFeBBe1IouS3"
   },
   "outputs": [],
   "source": [
    "### Defining higher-order Tensors ###\n",
    "\n",
    "'''TODO: Define a 2-d Tensor'''\n",
    "matrix = tf.zeros(shape=(2,4),dtype=tf.int32)\n",
    "\n",
    "assert isinstance(matrix, tf.Tensor), \"matrix must be a tf Tensor object\"\n",
    "assert tf.rank(matrix).numpy() == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Zv1fTn_Ya_cz"
   },
   "outputs": [],
   "source": [
    "'''TODO: Define a 4-d Tensor.'''\n",
    "# Use tf.zeros to initialize a 4-d Tensor of zeros with size 10 x 256 x 256 x 3. \n",
    "#   You can think of this as 10 images where each image is RGB 256 x 256.\n",
    "images = tf.zeros(shape=(10,256,256,3),dtype=tf.int32)\n",
    "\n",
    "assert isinstance(images, tf.Tensor), \"matrix must be a tf Tensor object\"\n",
    "assert tf.rank(images).numpy() == 4, \"matrix must be of rank 4\"\n",
    "assert tf.shape(images).numpy().tolist() == [10, 256, 256, 3], \"matrix is incorrect shape\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iD3VO-LZYZ2z"
   },
   "source": [
    "## 1.2 Computations on Tensors\n",
    "\n",
    "A convenient way to think about and visualize computations in TensorFlow is in terms of graphs. We can define this graph in terms of Tensors, which hold data, and the mathematical operations that act on these Tensors in some order. Let's look at a simple example, and define this computation using TensorFlow:\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/aamini/introtodeeplearning/master/lab1/img/add-graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_YJrZsxYZ2z"
   },
   "outputs": [],
   "source": [
    "# Create the nodes in the graph, and initialize values\n",
    "a = tf.constant(15)\n",
    "b = tf.constant(61)\n",
    "\n",
    "# Add them!\n",
    "c1 = tf.add(a,b)\n",
    "c2 = a + b # TensorFlow overrides the \"+\" operation so that it is able to act on Tensors\n",
    "print(c1)\n",
    "print(c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mbfv_QOiYZ23"
   },
   "source": [
    "Notice how we've created a computation graph consisting of TensorFlow operations, and how  the output is a Tensor with value 76 -- we've just created a computation graph consisting of operations, and it's executed them and given us back the result.\n",
    "\n",
    "Now let's consider a slightly more complicated example:\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/aamini/introtodeeplearning/master/lab1/img/computation-graph.png)\n",
    "\n",
    "Here, we take two inputs, `a, b`, and compute an output `e`. Each node in the graph represents an operation that takes some input, does some computation, and passes its output to another node.\n",
    "\n",
    "Let's define a simple function in TensorFlow to construct this computation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "PJnfzpWyYZ23",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Defining Tensor computations ###\n",
    "\n",
    "# Construct a simple computation function\n",
    "def func(a,b):\n",
    "  '''TODO: Define the operation for c, d, e (use tf.add, tf.subtract, tf.multiply).'''\n",
    "  c = tf.add(a,b)\n",
    "  d = tf.subtract(b,1)\n",
    "  e = tf.multiply(c,d)\n",
    "  return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwrRfDMS2-oy"
   },
   "source": [
    "Now, we can call this function to execute the computation graph given some inputs `a,b`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pnwsf8w2uF7p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Consider example values for a,b\n",
    "a, b = 1.5, 2.5\n",
    "# Execute the computation\n",
    "e_out = func(a,b)\n",
    "print(e_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HqgUIUhYZ29"
   },
   "source": [
    "Notice how our output is a Tensor with value defined by the output of the computation, and that the output has no shape as it is a single scalar value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tdkqk8pw5yJM"
   },
   "outputs": [],
   "source": [
    "### Gradient computation with GradientTape ###\n",
    "\n",
    "# y = x^2\n",
    "# Example: x = 3.0\n",
    "x = tf.Variable(3.0)\n",
    "\n",
    "# Initiate the gradient tape\n",
    "with tf.GradientTape() as tape:\n",
    "  # Define the function\n",
    "  y = x * x\n",
    "# Access the gradient -- derivative of y with respect to x\n",
    "dy_dx = tape.gradient(y, x)\n",
    "\n",
    "assert dy_dx.numpy() == 6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhU5metS5xF3"
   },
   "source": [
    "In training neural networks, we use differentiation and stochastic gradient descent (SGD) to optimize a loss function. Now that we have a sense of how `GradientTape` can be used to compute and access derivatives, we will look at an example where we use automatic differentiation and SGD to find the minimum of $L=(x-x_f)^2$. Here $x_f$ is a variable for a desired value we are trying to optimize for; $L$ represents a loss that we are trying to  minimize. While we can clearly solve this problem analytically ($x_{min}=x_f$), considering how we can compute this using `GradientTape` sets us up nicely for future labs where we use gradient descent to optimize entire neural network losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "attributes": {
     "classes": [
      "py"
     ],
     "id": ""
    },
    "id": "7g1yWiSXqEf-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing x=[[0.7042074]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'x value')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Function minimization with automatic differentiation and SGD ###\n",
    "\n",
    "# Initialize a random value for our initial x\n",
    "x = tf.Variable([tf.random.normal([1])])\n",
    "print(\"Initializing x={}\".format(x.numpy()))\n",
    "\n",
    "learning_rate = 1e-2 # learning rate for SGD\n",
    "history = []\n",
    "# Define the target value\n",
    "x_f = 4\n",
    "\n",
    "# We will run SGD for a number of iterations. At each iteration, we compute the loss, \n",
    "#   compute the derivative of the loss with respect to x, and perform the SGD update.\n",
    "for i in range(500):\n",
    "  with tf.GradientTape() as tape:\n",
    "    '''TODO: define the loss as described above'''\n",
    "    loss = (x - x_f) * (x - x_f)\n",
    "    # loss minimization using gradient tape\n",
    "    grad = tape.gradient(loss, x) # compute the derivative of the loss with respect to x\n",
    "    new_x = x - learning_rate*grad # sgd update\n",
    "    x.assign(new_x) # update the value of x\n",
    "    history.append(x.numpy()[0])\n",
    "\n",
    "# Plot the evolution of x as we optimize towards x_f!\n",
    "plt.plot(history)\n",
    "plt.plot([0, 500],[x_f,x_f])\n",
    "plt.legend(('Predicted', 'True'))\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('x value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC7czCwk3ceH"
   },
   "source": [
    "`GradientTape` provides an extremely flexible framework for automatic differentiation. In order to back propagate errors through a neural network, we track forward passes on the Tape, use this information to determine the gradients, and then use these gradients for optimization using SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3eI6DUic-6jo"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 MIT 6.S191\n",
    "# © MIT 6.S191: Introduction to Deep Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 A neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the tutorial we learned how to create a network model that predicts the handwritten digits from the MNIST dataset. This time we are trying recognize different items of clothing, trained from a dataset containing 10 different types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_n1U5do3u_F"
   },
   "source": [
    "The Fashion MNIST data is available directly in the tf.keras datasets API. You load it like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 Loading and Viewing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_n1U5do3u_F"
   },
   "source": [
    "The Fashion MNIST data is available directly in the tf.keras datasets API. \n",
    "- **Q** Load it like we did in the tutorial from keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmxkHFpt31bM"
   },
   "outputs": [],
   "source": [
    "mnist = #TODO\n",
    "(training_images, training_labels), (test_images, test_labels) = #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_n1U5do3u_F"
   },
   "source": [
    "The Fashion MNIST data is available directly in the tf.keras datasets API. \n",
    "- **Q** Normalize it like we did in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRH19pWs6ZDn"
   },
   "outputs": [],
   "source": [
    "training_images  = #TODO\n",
    "test_images = #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Q** Display 9 *random* images from the training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIn7S9gf62ie"
   },
   "source": [
    "Let's now design the model. Run the following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mAyndG3kVlK"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), \n",
    "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu), \n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BLMdl9aP8nQ0"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.optimizers.Adam(),\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rquQqIx4AaGR"
   },
   "source": [
    "Run the below code: It creates a set of classifications for each of the test images, and then prints the first entry in the classifications. The output, after you run it is a list of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RyEIki0z_hAD"
   },
   "outputs": [],
   "source": [
    "classifications = model.predict(test_images)\n",
    "\n",
    "print(classifications[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdzqbQhRArzm"
   },
   "source": [
    "Hint: try running print(test_labels[0]) -- and you'll get a 9. Does that help you understand why this list looks the way it does? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WnBGOrMiA1n5"
   },
   "outputs": [],
   "source": [
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUs7eqr7uSvs",
    "tags": []
   },
   "source": [
    "- **Q** What does this list represent?\n",
    "\n",
    "\n",
    "1.   It's 10 random meaningless values\n",
    "2.   It's the first 10 classifications that the computer made\n",
    "3.   It's the probability that this item is each of the 10 classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAbr92RTA67u"
   },
   "source": [
    "- TODO (Answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CD4kC6TBu-69"
   },
   "source": [
    "**Q** How do you know that this list tells you that the item is an ankle boot?\n",
    "\n",
    "\n",
    "1.   There's not enough information to answer that question\n",
    "2.   The 10th element on the list is the biggest, and the ankle boot is labelled 9\n",
    "2.   The ankle boot is label 9, and there are 0->9 elements in the list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAbr92RTA67u"
   },
   "source": [
    "- TODO (Answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgQSIfDSOWv6"
   },
   "source": [
    "Let's now look at the layers in your model. Experiment with different values for the dense layer with 512 neurons. What different results do you get for loss, training time etc? Why do you think that's the case? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOOEnHZFv5cS"
   },
   "source": [
    "**Q** Increase to 1024 Neurons -- What's the impact?\n",
    "\n",
    "1. Training takes longer, but is more accurate\n",
    "2. Training takes longer, but no impact on accuracy\n",
    "3. Training takes the same time, but is more accurate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAbr92RTA67u"
   },
   "source": [
    "- TODO (Answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GSZSwV5UObQP"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(training_images, training_labels) ,  (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "training_images = training_images/255.0\n",
    "test_images = test_images/255.0\n",
    "\n",
    "model = #TODO#\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy')\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "model.evaluate(test_images, test_labels)\n",
    "\n",
    "classifications = model.predict(test_images)\n",
    "\n",
    "print(classifications[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WtWxK16hQxLN"
   },
   "source": [
    "**Q** What would happen if you remove the Flatten() layer. Why do you think that's the case? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAbr92RTA67u"
   },
   "source": [
    "- TODO (Answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ExNxCwhcQ18S"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(training_images, training_labels) ,  (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "training_images = training_images/255.0\n",
    "test_images = test_images/255.0\n",
    "\n",
    "model = #TODO\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy')\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "model.evaluate(test_images, test_labels)\n",
    "\n",
    "classifications = model.predict(test_images)\n",
    "\n",
    "print(classifications[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqoCR-ieSGDg"
   },
   "source": [
    "**Q** Consider the final (output) layers. Why are there 10 of them? What would happen if you had a different amount than 10? For example, try training the network with 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAbr92RTA67u"
   },
   "source": [
    "- TODO (Answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMckVntcSPvo"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(training_images, training_labels) ,  (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "training_images = training_images/255.0\n",
    "test_images = test_images/255.0\n",
    "\n",
    "model = #TODO\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy')\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "model.evaluate(test_images, test_labels)\n",
    "\n",
    "classifications = model.predict(test_images)\n",
    "\n",
    "print(classifications[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HS3vVkOgCDGZ"
   },
   "source": [
    "**Q** Before you trained, you normalized the data, going from values that were 0-255 to values that were 0-1. What would be the impact of removing that? Here's the complete code to give it a try. Why do you think you get different results? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAbr92RTA67u"
   },
   "source": [
    "- TODO (Answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDqNAqrpCNg0"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "training_images=#TODO\n",
    "test_images=#TODO\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "model.evaluate(test_images, test_labels)\n",
    "classifications = model.predict(test_images)\n",
    "print(classifications[0])\n",
    "print(test_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "WBk0ZDWY-ff8"
   ],
   "name": "Part1_TensorFlow.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
