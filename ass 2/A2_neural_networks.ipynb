{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2fed8c8-3b58-464a-b89d-df85d934f740",
   "metadata": {},
   "source": [
    "---\n",
    "# Cairo University Faculty of Engineering\n",
    "## Deep Learning \n",
    "## Assignment 2\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f09594d-0c42-4205-a0af-77097e41f555",
   "metadata": {},
   "source": [
    "Please write your full name here\n",
    "- **Name** : \"amr mohamed ali\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dc8d6e-7d9a-48c0-a84d-fdd3c63c9c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from d2l import tensorflow as d2l\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6055f2a1-24c2-46eb-92a8-b9f387f3dcc2",
   "metadata": {
    "id": "1h4o9Bb0YZ29",
    "tags": []
   },
   "source": [
    "# Part 1 Computational Graphs\n",
    "\n",
    "**Motivation**. In this section we will develop expertise with an intuitive understanding of **backpropagation**, which is a way of computing gradients of expressions through recursive application of **chain rule**.\n",
    "\n",
    "Considering the example of a simple perceptron defined by just one dense layer: $ y = \\sigma(Wx + b)$, where $W$ represents a matrix of weights, $b$ is a bias, $x$ is the input, $\\sigma$ is the sigmoid activation function, and $y$ is the output. We can visualize this operation using a graph: \n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/aamini/introtodeeplearning/master/lab1/img/computation-graph-2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea98a23-c5be-4a87-8a53-588fbdcadf1e",
   "metadata": {},
   "source": [
    "## Example\n",
    "Suppose that we have a function of the form:\n",
    "\n",
    "$$\n",
    "f(x,y) = \\frac{x + \\sigma(y)}{\\sigma(x) + (x+y)^2}\n",
    "$$\n",
    "\n",
    "It is very important to stress that if you were to launch into performing the differentiation with respect to either \\\\(x\\\\) or \\\\(y\\\\), you would end up with very large and complex expressions. However, doing so is completely unnecessary because we don't need to have an explicit function written down that evaluates the gradient. We only have to know how to compute it.\n",
    "\n",
    "![Circ1](./img/circ1.svg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95e6d93-ec11-40d6-a639-40fad6e1ad3c",
   "metadata": {},
   "source": [
    "1. **Compute the forward path for this function. Use intermediate variables as shown above**\n",
    "\n",
    "- f --> final output\n",
    "- You can use math.exp for sigmoid calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76df1ec0-2969-4df1-98ac-0e1031ec9feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 3 # example values\n",
    "y = -4\n",
    "\n",
    "# forward pass\n",
    "\n",
    "### START CODE HERE ### (≈ 8 lines of code)\n",
    "\n",
    "\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1344fc6-02dd-4d1e-bd64-1768bac05469",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('f = %d ' % (f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2b9b8d-7afc-443b-a50f-a2ef68f4d7b9",
   "metadata": {},
   "source": [
    "Computing the backprop pass is easy: We’ll go backwards and for every variable along the way in the forward pass `sigy, num, sigx, xpy, xpysqr, den, invden` we will have the same variable, but one that begins with a `d`, which will hold the gradient of the output of the expresion with respect to that variable. Additionally, note that every single piece in our backprop will involve computing the local gradient of that expression, and chaining it with the gradient on that expression with a multiplication. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902480a7-16e1-415e-bdea-24fe845774fe",
   "metadata": {},
   "source": [
    "\n",
    "2. **Compute the backward path for this function to get:**\n",
    "$$\n",
    "\\nabla f(x,y) = [ \\frac{\\partial f}{\\partial y}, \\frac{\\partial f}{\\partial x} ]\n",
    "$$\n",
    "\n",
    "- **Use intermediate variables**\n",
    "- **Print dy and dx**\n",
    "- *Hint: Gradients add up at forks. The forward expression involves the variables x,y multiple times, so when we perform backpropagation we must be careful to use += instead of = to accumulate the gradient on these variables (otherwise we would overwrite it). This follows the multivariable chain rule in Calculus, which states that if a variable branches out to different parts, then the gradients that flow back to it will add.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc2bea2-601e-4e8f-8306-f5492550c590",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 12 lines of code)\n",
    "\n",
    "\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02334654-1d04-4f38-a8bf-e31e315cca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('df/dx = %d ' % (dx))\n",
    "print ('df/dy = %d ' % (dy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f383a40-2b6d-4bb3-bb45-4b6602896480",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 2: Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddb4513-a883-475c-acb4-7daf8a1434a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "origin_pos": 0,
    "tags": []
   },
   "source": [
    "In this part, (**we will implement the entire linear regression method from scratch,\n",
    "including the data pipeline, the model,\n",
    "the loss function, and the minibatch stochastic gradient descent optimizer.**)\n",
    "You will rely only on tensors and auto differentiation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9885bf7b-0575-456d-a16c-997636bb6b3b",
   "metadata": {},
   "source": [
    "we will use $n$ to denote\n",
    "the number of examples in our dataset.\n",
    "We index the data examples by $i$, denoting each input\n",
    "as $\\mathbf{x}^{(i)} = [x_1^{(i)}, x_2^{(i)}]^\\top$\n",
    "and the corresponding label as $y^{(i)}$.\n",
    "\n",
    "\n",
    "**Linear Model**\n",
    "\n",
    "When our inputs consist of $d$ features,\n",
    "we express our prediction $\\hat{y}$ (in general the \"hat\" symbol denotes estimates) as\n",
    "\n",
    "$$\\hat{y} = w_1  x_1 + ... + w_d  x_d + b.$$\n",
    "\n",
    "\n",
    "We will often find it convenient\n",
    "to refer to features of our entire dataset of $n$ examples\n",
    "via the *design matrix* $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$.\n",
    "Here, $\\mathbf{X}$ contains one row for every example\n",
    "and one column for every feature.\n",
    "\n",
    "For a collection of features $\\mathbf{X}$,\n",
    "the predictions $\\hat{\\mathbf{y}} \\in \\mathbb{R}^n$\n",
    "can be expressed via the matrix-vector product:\n",
    "\n",
    "$${\\hat{\\mathbf{y}}} = \\mathbf{X} \\mathbf{w} + b,$$\n",
    "\n",
    "\n",
    "**Loss Function**\n",
    "\n",
    "Determines a measure of *fitness*.\n",
    "The *loss function* quantifies the distance\n",
    "between the *real* and *predicted* value of the target.\n",
    "The most popular loss function in regression problems\n",
    "is the squared error.\n",
    "When our prediction for an example $i$ is $\\hat{y}^{(i)}$\n",
    "and the corresponding true label is $y^{(i)}$,\n",
    "the squared error is given by:\n",
    "\n",
    "$$l^{(i)}(\\mathbf{w}, b) = \\frac{1}{2} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2.$$\n",
    "\n",
    "\n",
    "A regression problem for a one-dimensional case\n",
    "as shown:\n",
    "\n",
    "![Fit data with a linear model.](./img/fit-linreg.svg)\n",
    "\n",
    "To measure the quality of a model on the entire dataset of $n$ examples,\n",
    "we simply average (or equivalently, sum)\n",
    "the losses on the training set.\n",
    "\n",
    "$$L(\\mathbf{w}, b) =\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\mathbf{w}, b) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2.$$\n",
    "\n",
    "When training the model, we want to find parameters ($\\mathbf{w}^*, b^*$)\n",
    "that minimize the total loss across all training examples:\n",
    "\n",
    "$$\\mathbf{w}^*, b^* = \\operatorname*{argmin}_{\\mathbf{w}, b}\\  L(\\mathbf{w}, b).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddedadc-6641-4c7a-8134-79549b655a62",
   "metadata": {},
   "source": [
    "**Minibatch Stochastic Gradient Descent**\n",
    "\n",
    "\n",
    "The key technique for optimizing nearly any deep learning model,\n",
    "and which we will call upon throughout this book,\n",
    "consists of iteratively reducing the error\n",
    "by updating the parameters in the direction\n",
    "that incrementally lowers the loss function --> *gradient descent*.\n",
    "\n",
    "We will often settle for sampling a random minibatch of examples\n",
    "every time we need to compute the update --> *minibatch stochastic gradient descent*.\n",
    "\n",
    "We can express the update mathematically as follows\n",
    "($\\partial$ denotes the partial derivative):\n",
    "\n",
    "$$(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{(\\mathbf{w},b)} l^{(i)}(\\mathbf{w},b).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b67706-78b0-428a-b984-03e8a44f2daf",
   "metadata": {},
   "source": [
    "## Synthetic Data, Simple Model\n",
    "### Generating the Dataset\n",
    "\n",
    "To keep things simple, we will [**construct an artificial dataset\n",
    "according to a linear model with additive noise.**]\n",
    "\n",
    "In the following code snippet, we generate a dataset\n",
    "containing 1000 examples, each consisting of 2 features\n",
    "sampled from a standard normal distribution.\n",
    "Thus our synthetic dataset will be a matrix\n",
    "$\\mathbf{X}\\in \\mathbb{R}^{1000 \\times 2}$.\n",
    "\n",
    "(**The true parameters generating our dataset will be\n",
    "$\\mathbf{w} = [2, -3.4]^\\top$ and $b = 4.2$,\n",
    "and**) our synthetic labels will be assigned according\n",
    "to the following linear model with the noise term $\\epsilon$:\n",
    "\n",
    "(**$$\\mathbf{y}= \\mathbf{X} \\mathbf{w} + b + \\mathbf\\epsilon.$$**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c56cf52-1cbe-4f80-9551-9988b28fe967",
   "metadata": {
    "origin_pos": 6,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "def synthetic_data(w, b, num_examples):  #@save\n",
    "    \"\"\"Generate y = Xw + b + noise.\"\"\"\n",
    "    X = tf.zeros((num_examples, w.shape[0]))\n",
    "    X += tf.random.normal(shape=X.shape)\n",
    "    y = tf.matmul(X, tf.reshape(w, (-1, 1))) + b\n",
    "    y += tf.random.normal(shape=y.shape, stddev=0.01)\n",
    "    y = tf.reshape(y, (-1, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7fc2ce-cdd3-45f0-b632-f5b7fbd38155",
   "metadata": {
    "origin_pos": 7,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(5)\n",
    "true_w = tf.constant([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a088e27d-495b-461f-b483-5eaaa97d1dc8",
   "metadata": {
    "origin_pos": 9,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "print('features:', features[0],'\\nlabel:', labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a27cb-fcac-4856-99cb-724fa649c9a9",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "By generating a scatter plot using the second feature `features[:, 1]` and `labels`,\n",
    "we can clearly observe the linear correlation between the two.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbcb65e-1376-4879-8a6c-5dc2dbc4c945",
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "d2l.set_figsize()\n",
    "# The semicolon is for displaying the plot only\n",
    "d2l.plt.scatter(features[:, (1)].numpy(), labels.numpy(), 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36253bc0-ba53-410c-86bb-7a2eb11f9592",
   "metadata": {
    "origin_pos": 5,
    "tags": []
   },
   "source": [
    "### Reading the Dataset\n",
    "\n",
    "In the following code [**call upon the existing API in a framework to read data.**]\n",
    "We pass in `features` and `labels` as arguments and specify `batch_size`\n",
    "when instantiating a data iterator object.\n",
    "Besides, the boolean value `is_train`\n",
    "indicates whether or not\n",
    "we want the data iterator object to shuffle the data\n",
    "on each epoch (pass through the dataset).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28419fa-707f-4a30-8d28-f18ab5d73c31",
   "metadata": {},
   "source": [
    "1. **Use the tf function from_tensor_slices to generate a tf dataset object with batch_size as input**\n",
    "2. **Use is_train flag to determine whether to shuffle the dataset or not**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aaab37-4b35-4ff3-9d72-292d260838a9",
   "metadata": {
    "origin_pos": 8,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True):  #@save\n",
    "    \"\"\"Construct a TensorFlow data iterator.\"\"\"\n",
    "    #### START YOUR CODE HERE ### ~ 4 LINES OF CODE\n",
    "    \n",
    "    # Read data\n",
    "    \n",
    "    # check on flag to shuffle or not\n",
    "    if is_train:\n",
    "        # shuffle dataset\n",
    "        \n",
    "    # Batch dataset\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1421cf52-54d8-4341-816c-a5d095bfb8c3",
   "metadata": {
    "origin_pos": 9,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e0c589-effb-40ba-8149-3309692fed48",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "3. **Use `iter` to construct a Python iterator and use `next` to obtain the first item from the iterator.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2144c8a5-c59c-4886-af3b-8f4353e8f7c6",
   "metadata": {
    "origin_pos": 11,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "### START YOUR CODE HERE ### ~ 1 LINE OF CODE\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24da4a06-3978-49c2-bcf4-cb4c1ae64f49",
   "metadata": {
    "origin_pos": 15
   },
   "source": [
    "4. **Explain what the output shape in the prvious tensors means:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eda004-0643-4132-b737-c1f2fc2049d9",
   "metadata": {
    "origin_pos": 16,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c83f595-3746-435e-a824-9b4011a5aa40",
   "metadata": {},
   "source": [
    "5. **How many batches are in the data_iter ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eb5391-71a5-45ef-8af3-8f6a33b7793c",
   "metadata": {
    "origin_pos": 16,
    "tab": [
     "tensorflow"
    ]
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71628cba-f449-4139-a02a-f4b9e6b53545",
   "metadata": {
    "origin_pos": 17,
    "tags": []
   },
   "source": [
    "### Initializing Model Parameters\n",
    "\n",
    "6. **Initialize weights by sampling random numbers from a normal distribution with mean 0 and a standard deviation of 0.01, and setting the bias to 0.**\n",
    "\n",
    "Note: For the shapes of the weights and bias, look at the generating a dataset part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db96e147-7256-4a1e-8c8a-6b89df19f117",
   "metadata": {
    "origin_pos": 20,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "## START YOUR CODE HERE ## ~ 2 lines of code\n",
    " \n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bbcd34-e487-4751-b8ff-a0f932350562",
   "metadata": {
    "origin_pos": 21
   },
   "source": [
    "### Defining the Model\n",
    "\n",
    "7. [**define our model, relating its inputs and parameters to its outputs.**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a892f99c-9846-4b4c-bf89-dd552ca2fd26",
   "metadata": {
    "origin_pos": 22,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "def linreg(X, w, b):  #@save\n",
    "    \"\"\"The linear regression model.\"\"\"\n",
    "    ## START YOUR CODE HERE ## ~ 1 line of code\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78acdc39-21c6-4125-b0de-b7a14a9c93eb",
   "metadata": {
    "origin_pos": 23
   },
   "source": [
    "### Defining the Loss Function\n",
    "\n",
    "8. (**define the loss function**): the squared loss function\n",
    "as described in Loss Function definition above.\n",
    "\n",
    "Note: In the implementation, you need to transform the true value `y`\n",
    "into the predicted value's shape `y_hat`.\n",
    "The result returned by the following function\n",
    "will also have the same shape as `y_hat`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e04bc2-0d67-4204-b0bf-b99ff8a1d0d3",
   "metadata": {
    "origin_pos": 24,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "def squared_loss(y_hat, y):  #@save\n",
    "    \"\"\"Squared loss.\"\"\"\n",
    "    ## START YOUR CODE HERE ## ~ 1 line of code\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212f6a87-1c81-4aa1-aebe-41334d719398",
   "metadata": {
    "origin_pos": 25
   },
   "source": [
    "### Defining the Optimization Algorithm\n",
    "\n",
    "At each step, using one minibatch randomly drawn from our dataset,\n",
    "we will estimate the gradient of the loss with respect to our parameters.\n",
    "Next, we will update our parameters\n",
    "in the direction that may reduce the loss.\n",
    "\n",
    "9. **Filll in the missing function below to apply the minibatch stochastic gradient descent update, given a set of parameters, a learning rate, and a batch size.**\n",
    "\n",
    "Note: use assign_sub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfe9ccd-271e-4fc3-ab9a-3720d6bf4b23",
   "metadata": {
    "origin_pos": 28,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "def sgd(params, grads, lr, batch_size):  #@save\n",
    "    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
    "    \n",
    "    ## START YOUR CODE HERE ## ~ 2 lines of code\n",
    "    \n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9016fa58-4d4b-44b1-91a1-6f1e8995c9f7",
   "metadata": {
    "origin_pos": 29
   },
   "source": [
    "### Training\n",
    "\n",
    "10. **Implement the following loop**\n",
    "\n",
    "* For each epoch :\n",
    "    * Initialize parameters $(\\mathbf{w}, b)$\n",
    "    * Repeat until done\n",
    "        * Compute gradient $\\mathbf{g} \\leftarrow \\partial_{(\\mathbf{w},b)} \\frac{1}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} l(\\mathbf{x}^{(i)}, y^{(i)}, \\mathbf{w}, b)$\n",
    "        * Update parameters $(\\mathbf{w}, b) \\leftarrow (\\mathbf{w}, b) - \\eta \\mathbf{g}$\n",
    "    * Print the loss at the end of each epoch using: `print(f'epoch {epoch + 1}, loss {float(tf.reduce_mean(train_l)):f}')`\n",
    "\n",
    "In each *epoch*,\n",
    "we will iterate through the entire dataset\n",
    "(using the `data_iter` function) once\n",
    "passing through every example in the training dataset\n",
    ".\n",
    "\n",
    "\n",
    "Set the number of epochs `num_epochs` and the learning rate `lr` to 3 and 0.03, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a93813-59c4-48e7-beaf-d27fb707a96f",
   "metadata": {
    "origin_pos": 30,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "lr = ####\n",
    "num_epochs = ###\n",
    "net = linreg\n",
    "loss = squared_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2d2d6e-1661-4737-b135-887221607b48",
   "metadata": {
    "origin_pos": 33,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "## START YOUR CODE HERE ## ~ 8 lines of code\n",
    "\n",
    "\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393f4953-35dc-46c1-b495-96172232bc3f",
   "metadata": {
    "origin_pos": 34
   },
   "source": [
    "In this case, because we synthesized the dataset ourselves,\n",
    "we know precisely what the true parameters are.\n",
    "\n",
    "11. [**evaluate our success in training by comparing the true parameters with those that we learned**] through our training loop. They should turn out to be very close to each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99daff3-745a-43fd-898e-bbf1e3804c78",
   "metadata": {
    "origin_pos": 35,
    "tab": [
     "tensorflow"
    ]
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##\n",
    "print(f'error in estimating w: {##############}')\n",
    "print(f'error in estimating b: {##############}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7b518a-c2ac-4f0e-941b-98d300ed9963",
   "metadata": {},
   "source": [
    "## Boston Housing Price Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b07d9c-b52e-4f52-8e70-f7a4bebd63bb",
   "metadata": {},
   "source": [
    "In this section, we’ll attempt to predict the median price of homes in a given Boston\n",
    "suburb in the mid-1970s, given data points about the suburb at the time, such as the\n",
    "crime rate, the local property tax rate, and so on. The dataset has few data points: only\n",
    "506, split between 404 training samples and 102 test samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cef42e-4494-4f16-b2d2-7cfe68c51e3d",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aea7770-afb6-45f2-93b1-b9c72df36bc0",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "1. **Load the Boston housing dataset from tf.keras.datasets into 2 tuples: (train_data, train_targets), (test_data, test_targets)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07dc3b6-c48e-4a37-8f73-3500f344e717",
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "## START YOUR CODE HERE ## ~ 2 lines of code\n",
    "\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6472180c-ebb8-44f9-8bab-c507e256ef39",
   "metadata": {},
   "source": [
    "2. **Look at the test and train data. Print their shapes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc2a160-b33f-44c5-83a7-08beb821a64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 4 lines of code)\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "print ('The shape of train_data is: ' + str(shape_X))\n",
    "print ('The shape of train_targets is: ' + str(shape_Y))\n",
    "print ('I have f = %d features!' % (f))\n",
    "print ('I have m = %d training examples!' % (m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4f798c-2c59-43a7-b1e0-d69bfb8cf140",
   "metadata": {},
   "source": [
    "3. **Search for the dataset features and write their names and information about each column:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb9e96c-c602-4ba9-b26b-3d12f336df06",
   "metadata": {},
   "source": [
    "**Answer**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ab30fe-d6b4-4733-a478-bd082d131b19",
   "metadata": {},
   "source": [
    "4. **Convert the train_data and the train_targets to pandas dataframes.**\n",
    "    * Use the 'columns' attribute so that the output dataframes have the column names you found in question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca4c3d0-280f-4f14-84e0-c9e807ef7b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 4 lines of code)\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c71f64-0215-460c-a685-aa10e172e49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147f68b4-cca9-43c7-b6c8-b0a063577b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f4ad51-a6a3-4cfc-9c64-6e7a44545d66",
   "metadata": {},
   "source": [
    "5. **What are the ranges of each column in features and the target column?**\n",
    "    * Use .describe method to find the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4d03cc-c705-4e4e-81f7-6d49a6e86fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60741722-c93c-4000-b024-efefd7c0d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code for Features ### (≈ 1 line of code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fad907-ae31-42f8-85ce-87ab68050013",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code for Target ### (≈ 1 line of code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742f762c-fe10-42be-a3d6-7cbb4df68c0d",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c701b66-acdf-4ed6-9f60-f6e498987510",
   "metadata": {},
   "source": [
    "As you noticed from the previous exercise each feature in the\n",
    "input data (for example, the crime rate) has a different scale. For instance, some values\n",
    "are proportions, which take values between 0 and 1, others take values between 1\n",
    "and 12, others between 0 and 100, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18cbdeb-252c-450b-b85a-d35d5989f928",
   "metadata": {},
   "source": [
    "It would be problematic to feed into a neural network values that all take wildly different\n",
    "ranges. A widespread best practice\n",
    "for dealing with such data is to do:\n",
    "\n",
    "- *feature-wise normalization*: for each feature in the input data (a column in the input data matrix), we subtract the mean of the feature and divide by the standard deviation, so that the feature is centered around 0 and has a unit standard deviation. This is easily done in NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e679d2b1-eded-4768-8954-c6cdfde3e207",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "6. **Use feature-wise normalization to normalize the data**\n",
    "\n",
    "Hint: Avoid leak between train and test data !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18246d4-dbd8-403a-9db9-a72360eaaebb",
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 6 lines of code)\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130ed50b-010c-488f-b1d0-aa2919fdf01d",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Building your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f72fab9-fb30-4396-8c22-f39637e2fdeb",
   "metadata": {},
   "source": [
    "Because so few samples are available, we’ll use a very small model with two intermediate\n",
    "layers, each with 64 units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dca53c6-d8c6-4a93-876e-080cb537e98a",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "7. **Use tf.keras.Sequential to build a model with:**\n",
    "    - 2 hidden layers each having 64 neurons and relu activation.\n",
    "    - 1 output layer\n",
    "    \n",
    "Hint: For output layer remember that this is a regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9bc68e-d3a4-42c6-b1e1-42c2c97fa651",
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eb30a2-e09f-4298-8210-998eb79eb7a9",
   "metadata": {},
   "source": [
    "Note that we compile the model with the mse loss function—mean squared error, the\n",
    "square of the difference between the predictions and the targets. We’re also monitoring a new metric during training: mean absolute error (MAE). It’s the\n",
    "absolute value of the difference between the predictions and the targets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a8739d-b009-43ee-9326-9e868c584893",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Validating your approach using K-fold validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06da8c95-c0d5-4ab1-8eab-01874dc9311e",
   "metadata": {},
   "source": [
    "To evaluate our model while we keep adjusting its parameters (such as the number of\n",
    "epochs used for training), we could split the data into a training set and a validation\n",
    "set. But because we have so few data points, the\n",
    "validation set would end up being very small (for instance, about 100 examples). As a\n",
    "consequence, the validation scores might change a lot depending on which data\n",
    "points we chose for validation and which we chose for training: the validation scores\n",
    "might have a high variance with regard to the validation split. This would prevent us\n",
    "from reliably evaluating our model.\n",
    "The best practice in such situations is to use K-fold cross-validation (see below figure)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35409c3e-22ce-48e4-acf1-bbef729b5dcd",
   "metadata": {},
   "source": [
    "![grayscale](https://deepchecks.com/wp-content/uploads/2022/01/evaluating_model_img9-1024x557.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2422387d-43f1-42ee-9b09-f02acad528d8",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "8. **Use model.fit to train the partial training data and validate validation data (val_data, val_targets) with num_epochs, and batch_sz**\n",
    "\n",
    "- Complete the k_fold_validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b630ac50-6392-4288-82db-893cd5af9da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_validation(train_data, train_targets, k = 4, num_epochs = 40, batch_sz = 16):\n",
    "    \n",
    "    num_val_samples = len(train_data) // k\n",
    "    all_mae_histories = []\n",
    "\n",
    "    for i in range(k):\n",
    "        print(f\"Processing fold #{i}\")\n",
    "\n",
    "        # Prepares the validation data: data from partition #k\n",
    "        val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "        val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "        # Prepares the training data: data from all other partitions\n",
    "        partial_train_data = np.concatenate(\n",
    "            [train_data[:i * num_val_samples],\n",
    "             train_data[(i + 1) * num_val_samples:]],\n",
    "            axis=0)\n",
    "        partial_train_targets = np.concatenate(\n",
    "            [train_targets[:i * num_val_samples],\n",
    "             train_targets[(i + 1) * num_val_samples:]],\n",
    "            axis=0)\n",
    "\n",
    "        # Builds the Keras model (already compiled)\n",
    "        model = build_model()\n",
    "\n",
    "\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        \n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        mae_history = history.history[\"val_mae\"]\n",
    "        all_mae_histories.append(mae_history)\n",
    "    return all_mae_histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aca8caf-836f-4418-bcf4-468ce48fb6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40\n",
    "batch_sz = 16\n",
    "all_mae_histories = k_fold_validation(train_data = train_data, train_targets = train_targets, k = 4, num_epochs = num_epochs, batch_sz = batch_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b89e8f-1bc8-4619-8437-0e07b2947b25",
   "metadata": {},
   "source": [
    "The 'all_mae_histories' carries for each fold the mae for each epoch of 500 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2ae70e-4b6b-4ceb-a097-e81927088c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(all_mae_histories).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f614c10-6ccc-499a-86a2-093bc8fb9eb8",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Building the history of successive mean K-fold validation scores**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826cdcf3-f1ba-4f9c-bd34-ac7087a02b82",
   "metadata": {},
   "source": [
    "We can then compute the average of the per-epoch MAE scores for all folds.\n",
    "\n",
    "So the output here would be the average mae for each epoch of training across different data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db436971-c5ad-4530-a187-6cde535b5a7b",
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "average_mae_history = [\n",
    "    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd39508-a0c0-4a1c-b6db-a579b1956da9",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Plotting validation scores, excluding the first 10 data points**\n",
    "\n",
    "It may be a little difficult to read the plot, due to a scaling issue: the validation MAE\n",
    "for the first few epochs is dramatically higher than the values that follow. Let’s omit\n",
    "the first 10 data points, which are on a different scale than the rest of the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e40867-3fb1-44dd-b862-436f27d53aeb",
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 8)\n",
    "\n",
    "truncated_mae_history = average_mae_history[10:]\n",
    "plt.plot(range(1, len(truncated_mae_history) + 1), truncated_mae_history)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation MAE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2539b4f-8829-4df1-9a45-a9c438a37baf",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2a596f-9e54-475c-9576-3aa89bf891b5",
   "metadata": {},
   "source": [
    "#### Epochs\n",
    "Let's try training for longer maybe we would get a better output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4f8d6e-a2e1-429e-8f5c-005965a631ba",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "9. **Use the function from question 8 and try training for 500 epochs, then plot MAE vs 500 epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb08eb7-ab31-4d95-b4b7-59480603bf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 3 or 1 lines of code)\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b853077-6533-4265-b495-3bedbd3371b9",
   "metadata": {},
   "source": [
    "The 'all_mae_histories' carries for each fold the mae for each epoch of 500 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19cd4ee-5db0-401c-bf8d-4ebfb305d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(all_mae_histories).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e4ab36-a163-450e-8496-64b215cf759d",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Building the history of successive mean K-fold validation scores**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfaed38-3976-4135-9765-75d01153ec79",
   "metadata": {},
   "source": [
    "We can then compute the average of the per-epoch MAE scores for all folds.\n",
    "\n",
    "So the output here would be the average mae for each epoch of training across different data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbaf641-4c48-468b-8aed-f40e4ec1597f",
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "average_mae_history = [\n",
    "    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc451ae8-7394-4f0f-b93d-a8642abb4415",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Plotting validation scores, excluding the first 10 data points**\n",
    "\n",
    "It may be a little difficult to read the plot, due to a scaling issue: the validation MAE\n",
    "for the first few epochs is dramatically higher than the values that follow. Let’s omit\n",
    "the first 10 data points, which are on a different scale than the rest of the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be82c10-73ca-4092-8e02-cd5bc9122059",
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 8)\n",
    "\n",
    "truncated_mae_history = average_mae_history[10:]\n",
    "plt.plot(range(1, len(truncated_mae_history) + 1), truncated_mae_history)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation MAE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48291ae-46b1-4188-8036-8577c41d13e9",
   "metadata": {},
   "source": [
    "10. **What epoch size would you pick? Why?**\n",
    "\n",
    "- Hint: When do we start overfitting?\n",
    "- Note: We truncate the first 10 epochs!!\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "As you can see in above, validation MAE stops improving significantly after\n",
    "120–140 epochs (this number includes the 10 epochs we omitted). Past that point,\n",
    "we start overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb875222-fd78-4ed5-a33f-a9c085bd5534",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2c8424-0b59-45be-8bc3-0f310fb8ed30",
   "metadata": {},
   "source": [
    "##### Size =  4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77369330-70e7-41eb-a8ff-40eae827ea7d",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "10. **Use the function from question 8 and try training for 4 batch size using best number of epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f37ea8-9290-4348-ad7e-f389756e1eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 3 or 1 lines of code)\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b778a6e-92f9-4da3-ae39-7ccad7d68eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(all_mae_histories).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98f8561-40a4-4766-95db-b8fd07b2b3fe",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Building the history of successive mean K-fold validation scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8270fb7f-7cb3-4ef7-a595-36db013fe37c",
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "average_mae_history = [\n",
    "    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8649b3a6-a3ab-4a6c-a1de-c195da01a2d2",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Plotting validation scores, excluding the first 10 data points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d71a2d3-eecd-48aa-ba24-bde5f56f430c",
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 8)\n",
    "\n",
    "truncated_mae_history = average_mae_history[10:]\n",
    "plt.plot(range(1, len(truncated_mae_history) + 1), truncated_mae_history)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation MAE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca24463c-6d6d-4dce-b365-1e0a10cbb6b9",
   "metadata": {},
   "source": [
    "##### Size =  16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6eeb57-5678-4309-8e8e-2de77eaeef9f",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "11. **Use the function from question 8 and try training for 16 batch size using best number of epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b18f56-c9af-48d4-9047-679dae4262b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 3 or 1 lines of code)\n",
    "\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e421ca-d258-41df-80e5-74b88f212bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(all_mae_histories).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c821339-16c6-4e3d-8298-456707e15b27",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Building the history of successive mean K-fold validation scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d21aa0c-c94d-438a-b0fc-dfc7ec63ac32",
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "average_mae_history = [\n",
    "    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efcd35a-3ca6-4d63-90a0-cc55ad75fbab",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Plotting validation scores, excluding the first 10 data points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1155359b-916f-4445-99a1-9b82d1854788",
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 8)\n",
    "\n",
    "truncated_mae_history = average_mae_history[10:]\n",
    "plt.plot(range(1, len(truncated_mae_history) + 1), truncated_mae_history)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation MAE\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c4cd41-4aa3-4a8e-a6bb-3c7862c25787",
   "metadata": {},
   "source": [
    "##### Size =  64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac6c4ea-9d90-4999-a9dc-fe5f2ef403bf",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "12. **Use the function from question 8 and try training for 64 batch size using best number of epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba055180-5a1e-4365-8eb0-e8a6f1aa495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 3 or 1 lines of code)\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2039e8fa-e3c4-4a69-b59e-44d8a5d55fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(all_mae_histories).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1349b349-6dc5-4f71-a7f5-94d1b510ede4",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Building the history of successive mean K-fold validation scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8f077f-4f6e-4f67-8c45-3fedb622f37e",
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "average_mae_history = [\n",
    "    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fe8e23-7722-495d-8318-8304d7265865",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Plotting validation scores, excluding the first 10 data points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252a4a9f-0815-4b0d-ad60-b12b5a3a70d1",
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 10)\n",
    "\n",
    "truncated_mae_history = average_mae_history[10:]\n",
    "plt.plot(range(1, len(truncated_mae_history) + 1), truncated_mae_history)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation MAE\")\n",
    "plt.yticks(np.arange(2,np.max(truncated_mae_history),.2))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58d23e3-89c6-4596-80d3-33b98d895eb1",
   "metadata": {},
   "source": [
    "10. **What effect does changing epoch size have on learning? Choose a batch size and the reason for your choice**\n",
    "\n",
    "**Answer:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aa55d1-2bc2-46d6-8088-c86f868f55aa",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Training the final model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c40115d-333c-47ed-9228-361a3e6fa7fe",
   "metadata": {},
   "source": [
    "Once you’re finished tuning other parameters of the model (in addition to the\n",
    "number of epochs), you can\n",
    "train a final production model on all of the training data, with the best parameters,\n",
    "and then look at its performance on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56afebbc-17f4-47c9-8c80-7092be7ed5e6",
   "metadata": {},
   "source": [
    "14. **Build and Train a final model on the entire training data then evaluate it on the test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee815ff2-052d-481f-9630-43d16224c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Your code\n",
    "best_batch = ##########\n",
    "best_epoch = #########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85250a7a-10a6-4164-9a8b-5d10ca81ccf1",
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### (~ 3 lines of code)\n",
    "\n",
    "## Build model\n",
    "\n",
    "## Train model\n",
    "\n",
    "## Evaluate model on test data\n",
    "\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1e4b2d-d567-45e4-81fb-54411188079a",
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "test_mae_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b66efe8-92bf-442f-86f0-1cf6b64061f9",
   "metadata": {},
   "source": [
    "15. **Interpret the test_mae_score. How many thousands of dollars are we off by?**\n",
    "\n",
    "\n",
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9df284f-7f5b-40a7-9a19-938cf65b29b2",
   "metadata": {},
   "source": [
    "**Optional**:\n",
    "\n",
    "Change the number of layers in the model and output the resulting learning curves."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
